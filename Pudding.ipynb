{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Pudding 2024\n",
    "***\n",
    "\n",
    "The purpose of this notebook will be to analyse data retrieved from the [Spotify Web API](https://developer.spotify.com/documentation/web-api) in order to train various machine learning models to predict the genre of any given song. Once the models have been trained, validated and tested, a function will be built that feeds the data from the API to the best preforming model, and it's genre will be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.char'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, cross_val_score, GridSearchCV, KFold\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/sklearn/__init__.py:84\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ]\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/sklearn/utils/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/sklearn/utils/_chunking.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/scipy/sparse/__init__.py:293\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/scipy/sparse/_base.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Base class for sparse matrices\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[1;32m      6\u001b[0m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[1;32m      7\u001b[0m                        matrix, validateaxis,)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[1;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misspmatrix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missparse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparray\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseWarning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseEfficiencyWarning\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/scipy/sparse/_sputils.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prod\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[1;32m     13\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupcast\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misscalarlike\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misintlike\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misshape\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missequence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misdense\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mismatrix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_sum_dtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m supported_dtypes \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mbool_, np\u001b[38;5;241m.\u001b[39mbyte, np\u001b[38;5;241m.\u001b[39mubyte, np\u001b[38;5;241m.\u001b[39mshort, np\u001b[38;5;241m.\u001b[39mushort, np\u001b[38;5;241m.\u001b[39mintc,\n\u001b[1;32m     17\u001b[0m                     np\u001b[38;5;241m.\u001b[39muintc, np_long, np_ulong, np\u001b[38;5;241m.\u001b[39mlonglong, np\u001b[38;5;241m.\u001b[39mulonglong,\n\u001b[1;32m     18\u001b[0m                     np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mlongdouble, \n\u001b[1;32m     19\u001b[0m                     np\u001b[38;5;241m.\u001b[39mcomplex64, np\u001b[38;5;241m.\u001b[39mcomplex128, np\u001b[38;5;241m.\u001b[39mclongdouble]\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/scipy/_lib/_util.py:18\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     Optional,\n\u001b[1;32m     12\u001b[0m     Union,\n\u001b[1;32m     13\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     14\u001b[0m     TypeVar,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, size \u001b[38;5;28;01mas\u001b[39;00m xp_size\n\u001b[1;32m     21\u001b[0m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[1;32m     22\u001b[0m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/scipy/_lib/_array_api.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnpt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     is_array_api_obj,\n\u001b[1;32m     23\u001b[0m     size,\n\u001b[1;32m     24\u001b[0m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[1;32m     25\u001b[0m     device\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray_namespace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_asarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/T7/Coding_Files/TripleTen Data Science/genre_predictor/my_forked_repo/.venv/lib/python3.10/site-packages/numpy/__init__.py:370\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current Numpy installation (\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m) fails to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass simple sanity checks. This can be caused for example \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby incorrect BLAS library being linked in, or by mixing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 370\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage managers (pip, conda, apt, ...). Search closed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy issues for similar problems.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__file__\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.char'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "### Spotify API Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the client_id and client_secret from environment variables\n",
    "client_id = os.getenv(\"Client_ID\")\n",
    "client_secret = os.getenv(\"Client_secret\")\n",
    "\n",
    "# Authentication\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager, requests_timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase timeout and add retry logic\n",
    "session = sp._session\n",
    "retry = Retry(\n",
    "    total=5,  # Total number of retries\n",
    "    backoff_factor=0.3,  # Wait time between retries\n",
    "    status_forcelist=[500, 502, 503, 504],  # Retry on these HTTP status codes\n",
    "    raise_on_status=False\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)  # Increase timeout to 10 seconds\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_genre_string(genre, debug=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        genre: str, genre string to test\n",
    "        debug: bool, print debug messages\n",
    "    Output:\n",
    "        bool: True if the genre is valid, False if not\n",
    "        \n",
    "    Tests if a genre string is valid by searching for tracks with that genre. If no tracks are found, the genre is invalid.\n",
    "    \"\"\"\n",
    "    results = sp.search(q=f'genre:{genre}', type='track', limit=1)\n",
    "    number_of_tracks = len(results['tracks']['items'])\n",
    "    if number_of_tracks == 0:\n",
    "        print(f\"No tracks found for genre: {genre}\")\n",
    "        return False\n",
    "    else:\n",
    "        if debug:\n",
    "            print(f\"Found {number_of_tracks} tracks for genre: {genre}\")\n",
    "        return True\n",
    "    \n",
    "def get_genres_of_interest(genres_dict, genre_record_limit, pagination_limit=45, debug=False):\n",
    "    \"\"\"\n",
    "    Fetches track IDs for specified genres from Spotify, ensuring a balanced representation of each genre.\n",
    "    Args:\n",
    "        genres_dict (dict): A dictionary where keys are super genres and values are lists of sub-genres.\n",
    "        genre_record_limit (int): The maximum number of tracks to fetch per super genre.\n",
    "        pagination_limit (int, optional): The number of tracks to fetch per API call. Defaults to 45.\n",
    "        debug (bool, optional): If True, prints debug information. Defaults to False.\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - track_ids (list): A list of track IDs fetched from Spotify.\n",
    "            - track_genre (list): A list of super genres corresponding to each track ID.\n",
    "    Raises:\n",
    "        AssertionError: If duplicate track IDs are found in the final list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables\n",
    "    track_ids = []\n",
    "    track_genre = []\n",
    "    seen_track_ids = set()\n",
    "\n",
    "    for super_genre, sub_genres in genres_dict.items():\n",
    "        print(f\"Getting records for super genre: {super_genre}\")\n",
    "        super_genre_track_ids = []\n",
    "\n",
    "        # Dictionary to track how many tracks we pulled per sub-genre\n",
    "        sub_genre_counts = {sub_genre: 0 for sub_genre in sub_genres}\n",
    "\n",
    "        # Loop until we hit the genre_record_limit for the super genre\n",
    "        total_tracks_pulled = 0\n",
    "        \n",
    "        while total_tracks_pulled < genre_record_limit and sub_genres:\n",
    "            # Calculate remaining tracks needed for the super genre\n",
    "            tracks_needed = genre_record_limit - total_tracks_pulled\n",
    "\n",
    "            # Shuffle sub-genres to randomize the pulls\n",
    "            random.shuffle(sub_genres)\n",
    "\n",
    "            for sub_genre in sub_genres[:]:\n",
    "                \n",
    "                # Adjust the batch size to ensure we don't exceed the genre_record_limit\n",
    "                batch_size = min(pagination_limit, tracks_needed)\n",
    "\n",
    "                # Fetch a batch of tracks for the sub-genre\n",
    "                results = sp.search(q=f'genre:{sub_genre}', type='track', limit=batch_size, offset=sub_genre_counts[sub_genre])\n",
    "                \n",
    "                # If no items are returned, remove the sub-genre and move on\n",
    "                if not results['tracks']['items']:\n",
    "                    if debug:\n",
    "                        print(f\"No items for sub-genre: {sub_genre}, removing from sub-genres\")\n",
    "                    sub_genres.remove(sub_genre)  # Remove sub-genre if no more tracks are returned\n",
    "                    continue  # Skip the rest of the code for this sub-genre\n",
    "            \n",
    "                # Add new track IDs that are not already seen, but ensure we don't exceed the genre_record_limit\n",
    "                new_track_ids = [track['id'] for track in results['tracks']['items'] if track['id'] not in seen_track_ids]\n",
    "                new_tracks_needed = genre_record_limit - total_tracks_pulled\n",
    "                \n",
    "                # Only add as many tracks as needed to reach the limit\n",
    "                new_track_ids = new_track_ids[:new_tracks_needed]\n",
    "                \n",
    "                for track_id in new_track_ids:\n",
    "                    # Add the new track to the super genre's collection\n",
    "                    super_genre_track_ids.append(track_id)\n",
    "                    track_genre.append(super_genre)  # Label the track with the super genre\n",
    "                    seen_track_ids.add(track_id)\n",
    "                \n",
    "                # Update counts and totals\n",
    "                sub_genre_counts[sub_genre] += len(new_track_ids)\n",
    "                total_tracks_pulled += len(new_track_ids)\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"Fetched {len(new_track_ids)} new tracks for sub-genre: {sub_genre}\")\n",
    "\n",
    "                # If we've reached the limit for the super genre, stop\n",
    "                if total_tracks_pulled >= genre_record_limit:\n",
    "                    break\n",
    "                \n",
    "            # Check again if we've exhausted all sub-genres\n",
    "            if not sub_genres:\n",
    "                if debug:\n",
    "                    print(f\"All sub-genres for super genre exhausted.\")\n",
    "                break\n",
    "\n",
    "        print(f\"{len(super_genre_track_ids)} records found for super genre: {super_genre}\\n\")\n",
    "        # Add the super genre track ids to the main list\n",
    "        track_ids.extend(super_genre_track_ids)\n",
    "\n",
    "    print(\"Total number of records:\", len(track_ids), \"\\n\")\n",
    "\n",
    "    # Check for duplicates in the final list\n",
    "    track_id_counts = Counter(track_ids)\n",
    "    duplicates = {track_id: count for track_id, count in track_id_counts.items() if count > 1}\n",
    "\n",
    "    # Assert no duplicates\n",
    "    assert not duplicates, f\"Duplicate track IDs found: {duplicates}\"\n",
    "\n",
    "    return track_ids, track_genre\n",
    "\n",
    "def get_other_genres(genres_of_interest, genre_record_limit, already_seen_ids, pagination_limit=45, debug=False):\n",
    "    \"\"\"\n",
    "    Fetches tracks from genres not included in the genres_of_interest.\n",
    "    Ensures an even distribution of tracks across genres and respects the genre_record_limit.\n",
    "    Args:\n",
    "        genres_of_interest (dict): A dictionary where keys are super genres and values are lists of sub-genres of interest.\n",
    "        genre_record_limit (int): The maximum number of tracks to fetch.\n",
    "        already_seen_ids (list or set): A list or set of track IDs that have already been gathered.\n",
    "        pagination_limit (int, optional): The number of tracks to fetch per API call. Defaults to 45.\n",
    "        debug (bool, optional): If True, prints debug information. Defaults to False.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - other_track_ids (list): A list of track IDs from the 'other' genres.\n",
    "            - genre_labels (list): A list of genre labels corresponding to the fetched track IDs.\n",
    "            - genre_counts (dict): A dictionary with genres as keys and the count of fetched tracks as values.\n",
    "    Raises:\n",
    "        AssertionError: If duplicate track IDs are found in the final list of track IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting 'other' genres.\")\n",
    "    already_seen_ids = set(already_seen_ids)  # Ensure it's a set for fast lookup\n",
    "\n",
    "    # Flatten the dictionary to get all sub-genres in genres_of_interest\n",
    "    sub_genres_of_interest = {sub_genre for super_genre, sub_genres in genres_of_interest.items() for sub_genre in sub_genres}\n",
    "\n",
    "    # All genres excluding genres_of_interest\n",
    "    other_genres = [genre for genre in sp.recommendation_genre_seeds()['genres'] if genre not in sub_genres_of_interest]\n",
    "\n",
    "    # Dictionary to track how many tracks we pulled per genre\n",
    "    genre_counts = {genre: 0 for genre in other_genres}\n",
    "\n",
    "    # List to hold unique track IDs for this function\n",
    "    other_track_ids = []\n",
    "\n",
    "    # Track total number of new tracks pulled\n",
    "    total_tracks_pulled = 0\n",
    "\n",
    "    # Loop until we hit the genre_record_limit\n",
    "    while total_tracks_pulled < genre_record_limit and other_genres:\n",
    "        # Calculate remaining tracks needed for the overall genre\n",
    "        tracks_needed = genre_record_limit - total_tracks_pulled\n",
    "\n",
    "        # Shuffle genres to randomize the pulls\n",
    "        random.shuffle(other_genres)\n",
    "\n",
    "        for genre in other_genres[:]:\n",
    "            # Adjust the batch size to ensure we don't exceed the genre_record_limit\n",
    "            batch_size = min(pagination_limit, tracks_needed)\n",
    "\n",
    "            # Fetch a batch of tracks for the genre\n",
    "            results = sp.search(q=f'genre:{genre}', type='track', limit=batch_size, offset=genre_counts[genre])\n",
    "\n",
    "            # If no items are returned, remove the genre and move on\n",
    "            if not results['tracks']['items']:\n",
    "                if debug:\n",
    "                    print(f\"No items for genre: {genre}, removing from other_genres\")\n",
    "                other_genres.remove(genre)\n",
    "                continue\n",
    "\n",
    "            # Add new track IDs that are not already seen, but ensure we don't exceed the genre_record_limit\n",
    "            new_track_ids = [track['id'] for track in results['tracks']['items'] if track['id'] not in already_seen_ids]\n",
    "            new_tracks_needed = genre_record_limit - total_tracks_pulled\n",
    "\n",
    "            # Only add as many tracks as needed to reach the limit\n",
    "            new_track_ids = new_track_ids[:new_tracks_needed]\n",
    "\n",
    "            for track_id in new_track_ids:\n",
    "                # Add the new track to the other track IDs collection\n",
    "                other_track_ids.append(track_id)\n",
    "                already_seen_ids.add(track_id)  # Also add to already seen IDs to avoid duplicates\n",
    "                genre_counts[genre] += 1\n",
    "                total_tracks_pulled += 1\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Fetched {len(new_track_ids)} new tracks for genre: {genre}\")\n",
    "\n",
    "            # If we've reached the genre record limit, stop\n",
    "            if total_tracks_pulled >= genre_record_limit:\n",
    "                break\n",
    "\n",
    "    print(f\"Total number of new records: {len(other_track_ids)} new tracks\")\n",
    "\n",
    "    if debug:\n",
    "        # Print genre counts only for genres with tracks > 0\n",
    "        print(f\"\\nGenre Counts:\")\n",
    "        for genre, count in genre_counts.items():\n",
    "            if count > 0:\n",
    "                print(f\"{genre}: {count}\")\n",
    "\n",
    "    # Create genre labels for the new tracks\n",
    "    genre_labels = [\"other\"] * len(other_track_ids)\n",
    "\n",
    "    # Check for duplicates in the final list of track IDs\n",
    "    track_id_counts = Counter(other_track_ids)\n",
    "    duplicates = {track_id: count for track_id, count in track_id_counts.items() if count > 1}\n",
    "\n",
    "    # Assert no duplicates\n",
    "    assert not duplicates, f\"Duplicate track IDs found: {duplicates}\"\n",
    "\n",
    "    return other_track_ids, genre_labels, genre_counts\n",
    "\n",
    "def amend_sub_genres(sub_genres):\n",
    "    \"\"\"\n",
    "    Input: A list of sub-genres\n",
    "    Output: The same list with any sub-genres removed that do not return results from the Spotify API\n",
    "    \"\"\"\n",
    "    sub_genres = sub_genres.copy()\n",
    "    # Modify the sub_genres list in place\n",
    "    before = len(sub_genres)\n",
    "    print(f\"Number of sub genres before check: {before}\")\n",
    "    \n",
    "    # Create a copy of the list to avoid modifying it while iterating\n",
    "    for genre in sub_genres[:]:\n",
    "        if not test_genre_string(genre):\n",
    "            sub_genres.remove(genre)\n",
    "            print(f\"Removed {genre} from sub_genres.\")\n",
    "    \n",
    "    print(f\"Number of sub genres after check: {len(sub_genres)}, {before - len(sub_genres)} removed.\")\n",
    "    \n",
    "    return sub_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Genres of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking sub-genres for rock\n",
      "Number of sub genres before check: 10\n",
      "Number of sub genres after check: 10, 0 removed.\n",
      "\n",
      "Checking sub-genres for pop\n",
      "Number of sub genres before check: 23\n",
      "Number of sub genres after check: 23, 0 removed.\n",
      "\n",
      "Checking sub-genres for rap/hip-hop\n",
      "Number of sub genres before check: 22\n",
      "No tracks found for genre: Lofi Hip Hop\n",
      "Removed Lofi Hip Hop from sub_genres.\n",
      "Number of sub genres after check: 21, 1 removed.\n",
      "\n",
      "Checking sub-genres for classical\n",
      "Number of sub genres before check: 22\n",
      "No tracks found for genre: Sacred Classical\n",
      "Removed Sacred Classical from sub_genres.\n",
      "No tracks found for genre: Cantata\n",
      "Removed Cantata from sub_genres.\n",
      "Number of sub genres after check: 20, 2 removed.\n",
      "\n",
      "Checking sub-genres for jazz\n",
      "Number of sub genres before check: 22\n",
      "No tracks found for genre: Modal Jazz\n",
      "Removed Modal Jazz from sub_genres.\n",
      "Number of sub genres after check: 21, 1 removed.\n"
     ]
    }
   ],
   "source": [
    "# Here you can add any string to any list in the dictionary. \n",
    "genres_of_interest = {\n",
    "    'rock': [\n",
    "            'rock',\n",
    "            'alt-rock',\n",
    "            'hard-rock',\n",
    "            'j-rock',\n",
    "            'psych-rock',\n",
    "            'punk-rock',\n",
    "            'rock-n-roll',\n",
    "            'rockabilly',\n",
    "            'grunge',\n",
    "            'punk'\n",
    "            ],\n",
    "    'pop': [\n",
    "            \"pop\",\n",
    "            \"Dance Pop\",\n",
    "            \"Electropop\",\n",
    "            \"Indie Pop\",\n",
    "            \"Synth-pop\",\n",
    "            \"Pop Rock\",\n",
    "            \"Teen Pop\",\n",
    "            \"Power Pop\",\n",
    "            \"Art Pop\",\n",
    "            \"Pop Punk\",\n",
    "            \"K-Pop\",\n",
    "            \"J-Pop\",\n",
    "            \"Latin Pop\",\n",
    "            \"Dream Pop\",\n",
    "            \"Bubblegum Pop\",\n",
    "            \"Euro Pop\",\n",
    "            \"Pop Rap\",\n",
    "            \"Chamber Pop\",\n",
    "            \"Baroque Pop\",\n",
    "            \"Pop Soul\",\n",
    "            \"Acoustic Pop\",\n",
    "            \"j-pop\",\n",
    "            \"k-pop\",\n",
    "            ],\n",
    "    'rap/hip-hop': [\n",
    "                \"Hip Hop\",\n",
    "                \"Hip-Hop\",\n",
    "                \"Rap\",\n",
    "                \"Trap\",\n",
    "                \"Gangsta Rap\",\n",
    "                \"East Coast Hip Hop\",\n",
    "                \"West Coast Hip Hop\",\n",
    "                \"Conscious Hip Hop\",\n",
    "                \"Alternative Hip Hop\",\n",
    "                \"Boom Bap\",\n",
    "                \"Dirty South\",\n",
    "                \"Crunk\",\n",
    "                \"Drill\",\n",
    "                \"Grime\",\n",
    "                \"Cloud Rap\",\n",
    "                \"Underground Hip Hop\",\n",
    "                \"Emo Rap\",\n",
    "                \"Hardcore Hip Hop\",\n",
    "                \"Lofi Hip Hop\",\n",
    "                \"Old School Hip Hop\",\n",
    "                \"Christian Hip Hop\",\n",
    "                \"Latin Hip Hop\"\n",
    "                ],\n",
    "    'classical': [\n",
    "                \"Classical\",\n",
    "                \"Baroque\",\n",
    "                \"Romantic\",\n",
    "                \"Classical\",\n",
    "                \"Chamber Music\",\n",
    "                \"Symphony\",\n",
    "                \"Opera\",\n",
    "                \"Choral\",\n",
    "                \"Contemporary Classical\",\n",
    "                \"Minimalism\",\n",
    "                \"Orchestral\",\n",
    "                \"Piano\",\n",
    "                \"String Quartet\",\n",
    "                \"Early Music\",\n",
    "                \"Renaissance\",\n",
    "                \"Modern Classical\",\n",
    "                \"Neoclassical\",\n",
    "                \"Impressionism\",\n",
    "                \"Avant-Garde\",\n",
    "                \"Sacred Classical\",\n",
    "                \"Cantata\",\n",
    "                \"Piano\"\n",
    "                ],\n",
    "    'jazz': [\n",
    "                \"Jazz\",\n",
    "                \"Bebop\",\n",
    "                \"Swing\",\n",
    "                \"Smooth Jazz\",\n",
    "                \"Cool Jazz\",\n",
    "                \"Hard Bop\",\n",
    "                \"Free Jazz\",\n",
    "                \"Fusion\",\n",
    "                \"Modal Jazz\",\n",
    "                \"Latin Jazz\",\n",
    "                \"Avant-Garde Jazz\",\n",
    "                \"Gypsy Jazz\",\n",
    "                \"Vocal Jazz\",\n",
    "                \"Jazz Funk\",\n",
    "                \"Jazz Blues\",\n",
    "                \"Soul Jazz\",\n",
    "                \"Post-Bop\",\n",
    "                \"Ragtime\",\n",
    "                \"Big Band\",\n",
    "                \"Dixieland\",\n",
    "                \"Nu Jazz\",\n",
    "                \"Jazz Fusion\",\n",
    "                ]\n",
    "}\n",
    "\n",
    "# This validates each string in the lists per super genre. If the string is not a recognized genre, it gets removed from the super genre list.\n",
    "\n",
    "for super_genre in genres_of_interest:\n",
    "    print(f\"\\nChecking sub-genres for {super_genre}\")\n",
    "    genres_of_interest[super_genre] = amend_sub_genres(genres_of_interest[super_genre])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval\n",
    "You can adjust the genre record limit, each super genre gets a maximum of genre_record_limit records.  \n",
    "Pagination is passed to the api as the limit parameter. Documentation says the max should be 50, but 45 seems to work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_record_limit = 1500\n",
    "pagination_limit = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting records for super genre: rock\n",
      "1500 records found for super genre: rock\n",
      "\n",
      "Getting records for super genre: pop\n",
      "1500 records found for super genre: pop\n",
      "\n",
      "Getting records for super genre: rap/hip-hop\n",
      "1500 records found for super genre: rap/hip-hop\n",
      "\n",
      "Getting records for super genre: classical\n",
      "1500 records found for super genre: classical\n",
      "\n",
      "Getting records for super genre: jazz\n",
      "1500 records found for super genre: jazz\n",
      "\n",
      "Total number of records: 7500 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "track_ids, track_genre = get_genres_of_interest(genres_of_interest, genre_record_limit, pagination_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert length of track_ids is equal to genre_record_limit * number of super genres\n",
    "assert len(track_ids) == genre_record_limit * len(genres_of_interest), f\"Expected {genre_record_limit * len(genres_of_interest)} tracks, but got {len(track_ids)}\"\n",
    "\n",
    "# Assert no duplicates\n",
    "assert len(track_ids) == len(set(track_ids)), \"Duplicate tracks found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 'other' genres.\n",
      "Total number of new records: 1500 new tracks\n"
     ]
    }
   ],
   "source": [
    "# Get other genres\n",
    "other_track_ids, other_genre_labels, other_genre_counts = get_other_genres(genres_of_interest, genre_record_limit, track_ids)\n",
    "\n",
    "# # This is helpful to see how many tracks were fetched for each super genre\n",
    "# for genre, count in other_genre_counts.items():\n",
    "#     if count > 0:\n",
    "#         print(f\"{genre}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two lists\n",
    "all_track_ids = track_ids + other_track_ids\n",
    "all_track_genre = track_genre + other_genre_labels\n",
    "\n",
    "# Create a DataFrame\n",
    "track_genres_df = pd.DataFrame({\"track_id\": all_track_ids, \"genre\": all_track_genre})\n",
    "\n",
    "# Assert no duplicates\n",
    "assert track_genres_df['track_id'].nunique() == len(track_genres_df), \"Duplicate track IDs found in the final DataFrame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching audio features: 100%|██████████| 200/200 [00:30<00:00,  6.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get audio features for each track with a progress bar\n",
    "track_features = []\n",
    "for i in tqdm(range(0, len(track_genres_df), pagination_limit), desc=\"Fetching audio features\"):\n",
    "    features = sp.audio_features(all_track_ids[i:i+pagination_limit])\n",
    "    \n",
    "    # Raise error if no features are returned\n",
    "    if not features:\n",
    "        raise ValueError(f\"No audio features returned for tracks: {all_track_ids[i:i+pagination_limit]}\")\n",
    "    \n",
    "    for feature in features:\n",
    "        # Raise error if no features are returned for individual tracks\n",
    "        if not feature:\n",
    "            raise ValueError(f\"No audio features returned for track: {all_track_ids[i:i+pagination_limit]}\")\n",
    "        track_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To DataFrame\n",
    "track_features_df = pd.DataFrame(track_features)\n",
    "# Rename id to track_id\n",
    "track_features_df.rename(columns={'id': 'track_id'}, inplace=True)\n",
    "\n",
    "# Assert no duplicates\n",
    "assert track_features_df['track_id'].nunique() == len(track_features_df), \"Duplicate track IDs found in the final DataFrame\"\n",
    "\n",
    "# Assert same length as track_genres_df\n",
    "assert len(track_features_df) == len(track_genres_df), \"Length of track_features_df and track_genres_df do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames\n",
    "all_data = pd.merge(track_genres_df, track_features_df, on='track_id')\n",
    "\n",
    "# # Save the data\n",
    "# all_data.to_csv(\"spotify_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9000 entries, 44AyOl4qVkzS48vBsbNXaC to 3tHCG0ISOA0pXscIdNrJml\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   genre             9000 non-null   object \n",
      " 1   danceability      9000 non-null   float64\n",
      " 2   energy            9000 non-null   float64\n",
      " 3   key               9000 non-null   int64  \n",
      " 4   loudness          9000 non-null   float64\n",
      " 5   mode              9000 non-null   int64  \n",
      " 6   speechiness       9000 non-null   float64\n",
      " 7   acousticness      9000 non-null   float64\n",
      " 8   instrumentalness  9000 non-null   float64\n",
      " 9   liveness          9000 non-null   float64\n",
      " 10  valence           9000 non-null   float64\n",
      " 11  tempo             9000 non-null   float64\n",
      " 12  type              9000 non-null   object \n",
      " 13  uri               9000 non-null   object \n",
      " 14  track_href        9000 non-null   object \n",
      " 15  analysis_url      9000 non-null   object \n",
      " 16  duration_ms       9000 non-null   int64  \n",
      " 17  time_signature    9000 non-null   int64  \n",
      "dtypes: float64(9), int64(4), object(5)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('spotify_data.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9000 entries, 44AyOl4qVkzS48vBsbNXaC to 3tHCG0ISOA0pXscIdNrJml\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   genre             9000 non-null   object \n",
      " 1   danceability      9000 non-null   float64\n",
      " 2   energy            9000 non-null   float64\n",
      " 3   key               9000 non-null   int64  \n",
      " 4   loudness          9000 non-null   float64\n",
      " 5   mode              9000 non-null   int64  \n",
      " 6   speechiness       9000 non-null   float64\n",
      " 7   acousticness      9000 non-null   float64\n",
      " 8   instrumentalness  9000 non-null   float64\n",
      " 9   liveness          9000 non-null   float64\n",
      " 10  valence           9000 non-null   float64\n",
      " 11  tempo             9000 non-null   float64\n",
      " 12  duration_ms       9000 non-null   int64  \n",
      " 13  time_signature    9000 non-null   int64  \n",
      "dtypes: float64(9), int64(4), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(['type', 'uri', 'track_href', 'analysis_url'], axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.duplicated().sum())\n",
    "print(data['track_id'].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial review shows our data has object and int/float columns. Object columns, aside from the genre, all can be dropped since they dont contribute to how genres are assigned. Int and float columns are labeled properly and can be used for training the model. There are no missing data and no duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list of attributes\n",
    "attributes = data.columns.drop('genre').tolist()\n",
    "\n",
    "# Violinplot of attributes per genre\n",
    "fig, axs = plt.subplots(2, 7, figsize=(20,10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, attribute in enumerate(attributes):\n",
    "    sns.violinplot(data, x=attribute, y='genre', hue='genre', ax=axs[i], palette=['#006450', '#477D95', '#90EDDA', '#7D4B32', '#8D67AB', '#777777'])\n",
    "    # Remove labels for figures not in first column\n",
    "    if i % 7 != 0:\n",
    "        axs[i].set_ylabel('')\n",
    "        axs[i].set_yticks([])\n",
    "\n",
    "fig.delaxes(axs[-1])\n",
    "plt.suptitle('Distribution of Attributes per Genre', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can see which attributes have a tendency to distiguish between genres (e.g. low energy score for classical music, higher speechiness for rap/hiphop), but we also want to verify if they are statistically different from each other per attribute so we can statistically determine whether to keep an attribute for the model training or not. If we were to just rely on visuals, we may drop key and mode due to indistinguishable differences, but let's use ANOVA to prove or disprove otherwise.\n",
    "\n",
    "We can also generate the grouped means of each attribute per genre to get a more numerical sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get grouped attribute means by genre\n",
    "grouped_means = data.groupby('genre').mean()\n",
    "grouped_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list for values\n",
    "attribute_stats = []\n",
    "\n",
    "# Perform ANOVA\n",
    "for attribute in attributes:\n",
    "    anova_results = stats.f_oneway(\n",
    "        data[data['genre'] == 'classical'][attribute],\n",
    "        data[data['genre'] == 'jazz'][attribute],\n",
    "        data[data['genre'] == 'pop'][attribute],\n",
    "        data[data['genre'] == 'rock'][attribute],\n",
    "        data[data['genre'] == 'rap/hip-hop'][attribute],\n",
    "        data[data['genre'] == 'other'][attribute],\n",
    "    )\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if anova_results.pvalue < alpha:\n",
    "        attribute_stats.append([attribute, anova_results.statistic, anova_results.pvalue, 'Yes'])\n",
    "    else:\n",
    "        attribute_stats.append([attribute, anova_results.statistic, anova_results.pvalue, 'No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_summary = pd.DataFrame(attribute_stats, columns=['Attribute', 'F-statistic', 'p-value', 'Significantly Different?'])\n",
    "attribute_summary = attribute_summary.set_index('Attribute')\n",
    "attribute_summary = attribute_summary.sort_values(by='F-statistic', ascending=False)\n",
    "attribute_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(attribute_summary, y='F-statistic', x='Attribute')\n",
    "plt.xticks(rotation=60)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the differences statistically, we can see based on the ANOVA test that there are some attributes that have more differences between each genre than the other, confirming what we were able to see visually in the violinplots. Additionally, the results of the key and mode attributes show that they are still significantly different per genre, although not as pronounced as the other attributes, but can still be used to distinguish between genres. All the attributes got a p-value less than 0.05, so we reject the null hypothesis that attributes per genre are not significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.iloc[:,1:].corr()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, linewidths=0.5, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid multicollinearity, we may need to omit some features/attributes that have a high correlation coefficient. Coupling this information with the F-statistic results, it's best to keep the attribute with the highest F-statistic since it has a better ratio of variance between the grouped means and variance within the group. This gives the model a better gauge of distinction between the classes and should improve our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Before spliting the data into training and test datasets, the data is scaled between -1 and 1 to provide optimal training conditions for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = data.drop(['genre', 'track_id'], axis=1)\n",
    "scaled_data = pd.DataFrame(scaler.fit_transform(scaled_data), index=scaled_data.index, columns=scaled_data.columns)\n",
    "data[scaled_data.columns] = scaled_data[scaled_data.columns]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8100, 14)\n",
      "(900, 14)\n",
      "(8100, 13) (8100,)\n",
      "(900, 13) (900,)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, random_state=42)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "X_train = train.drop(['genre'], axis=1)\n",
    "y_train = train['genre']\n",
    "X_test = test.drop(['genre'], axis=1)\n",
    "y_test = test['genre']\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single KFold instance will be used for training all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing KFold Instance\n",
    "cross_validator = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Model- Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dummy model param grid\n",
    "dummy_params = {\n",
    "    'strategy':['most_frequent', 'prior', 'stratified', 'uniform', 'constant'],\n",
    "    'constant':['rock']\n",
    "}\n",
    "\n",
    "# Initializing DummyClassifier and its GridSearchCV\n",
    "dummy_model = DummyClassifier(random_state=42)\n",
    "dummy_grid = GridSearchCV(dummy_model, dummy_params, scoring='f1_weighted', cv=cross_validator)\n",
    "\n",
    "# Training Grid\n",
    "dummy_grid.fit(X_train, y_train)\n",
    "dummy_roc_auc = cross_val_score(dummy_grid.best_estimator_, X_train, y_train, scoring='roc_auc_ovo_weighted', cv=cross_validator).mean()\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "dummy_pred = dummy_grid.best_estimator_.predict(X_train)\n",
    "dummy_cm = metrics.confusion_matrix(y_train, dummy_pred)\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(dummy_cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=y_train.value_counts().index, yticklabels=y_train.value_counts().index)\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('Dummy Model Confusion Matrix',fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(dummy_grid.best_estimator_)\n",
    "print(f'F1: {dummy_grid.best_score_}')\n",
    "print('ROC_AUC of Dummy model: ', dummy_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DecisionTree model param grid\n",
    "tree_params = {\n",
    "    'max_depth':np.arange(3, 11, 1),\n",
    "    'min_samples_split':[2,4,6]\n",
    "}\n",
    "\n",
    "# Initializing DecisionTree and its GridSearchCV\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_grid = GridSearchCV(tree_model, tree_params, scoring='f1_weighted', cv=cross_validator)\n",
    "\n",
    "# Training Grid\n",
    "tree_grid.fit(X_train, y_train)\n",
    "tree_roc_auc = cross_val_score(tree_grid.best_estimator_, X_train, y_train, scoring='roc_auc_ovo_weighted', cv=cross_validator).mean()\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "tree_pred = tree_grid.best_estimator_.predict(X_train)\n",
    "tree_cm = metrics.confusion_matrix(y_train, tree_pred)\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(tree_cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=y_train.value_counts().index, yticklabels=y_train.value_counts().index)\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('Decision Tree Model Confusion Matrix',fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(tree_grid.best_estimator_)\n",
    "print(f'F1: {tree_grid.best_score_}')\n",
    "print('ROC_AUC of Tree model: ', tree_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LightGBM model param grid\n",
    "lightgbm_params = {\n",
    "    'num_leaves':[31, 100, 200],\n",
    "    'learning_rate':[0.01]\n",
    "}\n",
    "\n",
    "# Initializing LightGBM and its GridSearchCV\n",
    "lightgbm_model = lgb.LGBMClassifier(random_state=42, verbosity=-1)\n",
    "lightgbm_grid = GridSearchCV(lightgbm_model, lightgbm_params, scoring='f1_weighted', cv=cross_validator)\n",
    "\n",
    "# Training Grid\n",
    "lightgbm_grid.fit(X_train, y_train)\n",
    "lightgbm_roc_auc = cross_val_score(lightgbm_grid.best_estimator_, X_train, y_train, scoring='roc_auc_ovo_weighted', cv=cross_validator).mean()\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "lightgbm_pred = lightgbm_grid.best_estimator_.predict(X_train)\n",
    "lightgbm_cm = metrics.confusion_matrix(y_train, lightgbm_pred)\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(lightgbm_cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=y_train.value_counts().index, yticklabels=y_train.value_counts().index)\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('Light GBM Model Confusion Matrix',fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(lightgbm_grid.best_estimator_)\n",
    "print(f'F1: {lightgbm_grid.best_score_}')\n",
    "print('ROC_AUC of LightGBM model: ', lightgbm_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CatBoost model param grid\n",
    "catboost_params = {\n",
    "    'iterations':[1001, 2001],\n",
    "    'learning_rate':[0.01]\n",
    "}\n",
    "\n",
    "# Initializing CatBoost and its GridSearchCV\n",
    "catboost_model = CatBoostClassifier(random_seed=42, verbose=1000)\n",
    "catboost_grid = GridSearchCV(catboost_model, catboost_params, scoring='f1_weighted', cv=cross_validator)\n",
    "\n",
    "# Training Grid\n",
    "catboost_grid.fit(X_train, y_train)\n",
    "catboost_roc_auc = cross_val_score(catboost_grid.best_estimator_, X_train, y_train, scoring='roc_auc_ovo_weighted', cv=cross_validator).mean()\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "catboost_pred = catboost_grid.best_estimator_.predict(X_train)\n",
    "catboost_cm = metrics.confusion_matrix(y_train, catboost_pred)\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(catboost_cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=y_train.value_counts().index, yticklabels=y_train.value_counts().index)\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('CatBoost Model Confusion Matrix',fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(catboost_grid.best_estimator_)\n",
    "print( f'F1: {catboost_grid.best_score_}')\n",
    "print('ROC_AUC of CatBoost model: ', catboost_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RandomForest model param grid\n",
    "forest_params = {\n",
    "    'n_estimators':[1000, 1500],\n",
    "    'max_depth':np.arange(18, 27, 2),\n",
    "}\n",
    "\n",
    "# Initializing RandomForest and its GridSearchCV\n",
    "forest_model = RandomForestClassifier(random_state=42)\n",
    "forest_grid = GridSearchCV(forest_model, forest_params, scoring='f1_weighted', cv=cross_validator)\n",
    "\n",
    "# Training Grid\n",
    "forest_grid.fit(X_train, y_train)\n",
    "forest_roc_auc = cross_val_score(forest_grid.best_estimator_, X_train, y_train, scoring='roc_auc_ovo_weighted', cv=cross_validator).mean()\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "forest_pred = forest_grid.best_estimator_.predict(X_train)\n",
    "forest_cm = metrics.confusion_matrix(y_train, forest_pred)\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(forest_cm, annot=True, fmt='d', cmap='YlGnBu', xticklabels=y_train.value_counts().index, yticklabels=y_train.value_counts().index)\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('Random Forest Model Confusion Matrix',fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(forest_grid.best_estimator_)\n",
    "print(f'F1: {forest_grid.best_score_}')\n",
    "print('ROC_AUC of Random Forest model: ', forest_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = forest_grid.best_estimator_\n",
    "\n",
    "# Predicting test dataset\n",
    "test_predict = test_model.predict(X_test)\n",
    "test_proba = test_model.predict_proba(X_test)\n",
    "test_roc_auc = metrics.roc_auc_score(y_test, test_proba, average='weighted', multi_class='ovo')\n",
    "test_f1 = metrics.f1_score(y_test, test_predict, average='weighted')\n",
    "test_cm = metrics.confusion_matrix(y_test, test_predict)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.heatmap(test_cm, annot=True,fmt='d', cmap='YlGnBu', xticklabels=y_train.value_counts().index, yticklabels=y_train.value_counts().index)\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('Test Model Confusion Matrix',fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(test_model)\n",
    "print('Test ROC_AUC:', test_roc_auc)\n",
    "print('Test F1 : ', test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_features(song_title, artist_name):\n",
    "    # Search for the song using Spotipy's search function\n",
    "    result = sp.search(q=f\"track:{song_title} artist:{artist_name}\", type='track', limit=1)\n",
    "    \n",
    "    if result['tracks']['items']:\n",
    "        # Extract the track ID from the search result\n",
    "        track = result['tracks']['items'][0]\n",
    "        track_id = track['id']\n",
    "        track_name = track['name']\n",
    "        artist_name = track['artists'][0]['name']\n",
    "        \n",
    "        print(f\"Found track: {track_name} by {artist_name}\")\n",
    "        \n",
    "        # Use the track ID to get the song's features\n",
    "        features = sp.audio_features(track_id)\n",
    "        return features[0]  # Return the features dictionary\n",
    "    else:\n",
    "        print(f\"No results found for {song_title} by {artist_name}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "song_title = \"Spybreak-Short One\"\n",
    "artist_name = \"Propellerheads\"\n",
    "features = get_track_features(song_title, artist_name)\n",
    "\n",
    "if features:\n",
    "    print(\"Audio Features:\")\n",
    "    print(features)\n",
    "\n",
    "# Removing columns & scaling data\n",
    "song_data = pd.DataFrame(features, index=[0], columns=features.keys()).drop(['type', 'uri', 'track_href', 'analysis_url', 'id'], axis=1)\n",
    "song_data[scaled_data.columns] = scaler.transform(song_data[scaled_data.columns])\n",
    "search_genre = test_model.predict(song_data)\n",
    "print(f'Genre of chosen track: {search_genre}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
